import argparse
import datetime
import json
import os.path
import pickle
import shutil
from concurrent.futures import ProcessPoolExecutor
from glob import glob
from pathlib import Path
from sys import path

import numpy as np

# append root dir to python path so that we find `sumo`
path.insert(0, str(Path(__file__).absolute().parents[1]))
from sumo.data import Subject
from sumo.data.data import (
    create_subjects,
    create_train_test,
    create_train_val,
    create_cv_folds,
)
from helper.a7_parallel_evaluation import split_evaluation


def get_args():
    parser = argparse.ArgumentParser(
        description="Create dataset splits, evaluate them on A7 algorithm and choose split"
        "with median performance of A7. Alternatively, you can define a predefined split (e.g., the split used in our paper).",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    default_input_dir = Path(__file__).absolute().parents[1] / "input"
    date_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    default_output_dir = (
        Path(__file__).absolute().parents[1] / "output" / f"datasets_{date_time}"
    )

    parser.add_argument(
        "-i",
        "--input",
        type=str,
        default=default_input_dir,
        help="Directory with stored input files as generated by the MODA file MODA02_genEEGVectBlock.m",
    )
    parser.add_argument(
        "--predefined_split",
        type=str,
        default="input/moda_datasplit.json",
        help="Path to a json file containing a predefined split. If this parameter is set, n_datasets is ignored.",
    )
    parser.add_argument(
        "-n",
        "--n_datasets",
        type=int,
        default=25,
        help="Number of data splits to create and evaluate wih A7, the split with median performance is chosen. Only used if no predefined_split is given.",
    )
    parser.add_argument(
        "-t",
        "--test",
        type=float,
        default=0.2,
        help="Proportion of the data that is used as test data (0-1). Only used if no predefined_split is given.",
    )
    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default=default_output_dir,
        help="Output directory to save generated data splits in",
    )

    return parser.parse_args()


def create_subjects_list():
    eeg_files = [str(input_dir / "EEGVect_p1.mat"), str(input_dir / "EEGVect_p2.mat")]
    spindle_files = ["moda/GCVect_exp_p1.mat", "moda/GCVect_exp_p2.mat"]
    spindle_files = [os.path.join(parent_dir, f) for f in spindle_files]
    block_files = ["moda/6_segListSrcDataLoc_p1.txt", "moda/7_segListSrcDataLoc_p2.txt"]
    block_files = [os.path.join(parent_dir, f) for f in block_files]

    subjects = create_subjects(eeg_files, spindle_files, block_files)

    return subjects


def create_random_splits():
    subjects = create_subjects_list()

    for i in range(args.n_datasets):
        subjects_train, subjects_test = create_train_test(subjects, args.test)
        output_dict = {"train": subjects_train.tolist(), "test": subjects_test.tolist()}
        with open(output_dir / f"split_{i:02d}.pickle", "wb") as output_file:
            pickle.dump(output_dict, output_file)


def choose_split():
    input_files = sorted(glob(str(output_dir / "split_*.pickle")))

    f1s = []
    with ProcessPoolExecutor() as pool:
        for f1_mean in pool.map(split_evaluation, input_files):
            f1s.append(f1_mean)

    # for an even number of splits use the split with the f1 score closest to the median
    final_split_idx = np.where(f1s == np.quantile(f1s, 0.5, interpolation="nearest"))[
        0
    ][0]
    shutil.copy(input_files[final_split_idx], output_dir / "final_split.pickle")

    return final_split_idx, f1s


if __name__ == "__main__":
    args = get_args()

    input_dir = Path(args.input)
    output_dir = Path(args.output)
    output_dir.mkdir(exist_ok=True)
    parent_dir = Path(__file__).absolute().parents[0]

    if os.path.exists(args.predefined_split):
        print(f"Using predefined split from {args.predefined_split}")
        predefined_split = json.load(open(args.predefined_split))

        subjects = create_subjects_list()
        subjects = np.concatenate(subjects)

        for split_type in predefined_split.keys():
            splits = predefined_split[split_type]
            output_dict = {"test": []}

            for split_name, split_subject_ids in splits.items():
                split_subjects = [
                    sub for sub in subjects if sub.patient_id in split_subject_ids
                ]
                if "test" in split_name:  # test splits for phases are grouped
                    output_dict["test"].append(split_subjects)
                else:
                    output_dict[split_name] = split_subjects

            with open(
                output_dir / f"subjects_{split_type}.pickle", "wb"
            ) as output_path:
                pickle.dump(output_dict, output_path)
    else:
        print("No predefined split given, creating random splits...")
        assert 0 <= args.test <= 1

        create_random_splits()  # create n_datasets splits as described in our paper
        split_idx, f1s = (
            choose_split()
        )  # evaluate A7 on the test data per split and choose split with median performance

        with open(output_dir / "final_split.pickle", "rb") as final_split_path:
            final_split = pickle.load(final_split_path)

        # create hold-out validation set, to demonstrate create_train_val function
        subjects_train, subjects_val = create_train_val(final_split["train"], 0.1)
        output_dict = {
            "train": subjects_train,
            "val": subjects_val,
            "test": final_split["test"],
        }
        with open(output_dir / "subjects_val.pickle", "wb") as output_path:
            pickle.dump(output_dict, output_path)

        # create cross validation folds, to demonstrate create_cv_folds function
        _, val_folds = create_cv_folds(np.array(final_split["train"], dtype=Subject), 6)
        output_dict = {}
        for i, val_fold in enumerate(val_folds):
            output_dict[f"fold_{i}"] = val_fold.tolist()
        output_dict["test"] = final_split["test"]
        with open(output_dir / "subjects_cv.pickle", "wb") as output_path:
            pickle.dump(output_dict, output_path)
